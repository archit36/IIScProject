{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQBF2DV9sjIGy1ZrLPvPI4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/archit36/IIScProject/blob/main/CapstoneProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Development and Optimization of Deep Q-Networks for Autonomous Lunar Lander Control"
      ],
      "metadata": {
        "id": "_ufMOJu-mT1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The project focuses on designing and optimizing a Deep Q-Network (DQN) to autonomously control a lunar lander in a simulated environment. The primary challenge is to enable the lander to reach its target safely by learning optimal policies for thrust control, orientation, and landing speed."
      ],
      "metadata": {
        "id": "l5RuWQ4ymmyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Update/Upgrade the system and install libs\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get install -y swig build-essential python-dev python3-dev > /dev/null 2>&1\n",
        "!apt-get install x11-utils > /dev/null 2>&1\n",
        "!apt-get install xvfb > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "tbgLr2wVlxfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install dependencies\n",
        "!pip install rarfile --quiet\n",
        "!pip install 'stable-baselines3[extra]' --quiet\n",
        "!pip install ale-py --quiet\n",
        "!pip install swig\n",
        "!pip install gym --quiet\n",
        "!pip install pyvirtualdisplay --quiet\n",
        "!pip install pyglet --quiet\n",
        "!pip install pygame --quiet\n",
        "!pip install minigrid --quiet\n",
        "!pip install -q swig --quiet\n",
        "!pip install -q gymnasium --quiet\n",
        "!pip install 'minigrid<=2.1.1' --quiet\n",
        "!pip3 install box2d-py --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70w1DkAa5O0m",
        "outputId": "3dec5378-1acb-4920-a331-57671a6cd3a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting swig\n",
            "  Using cached swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.6 kB)\n",
            "Using cached swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "Installing collected packages: swig\n",
            "Successfully installed swig-4.2.1\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m936.6/936.6 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing necessary libraries**"
      ],
      "metadata": {
        "id": "B4KOQW4NnLCF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "Pud-zKaU4Yu7"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import io\n",
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import base64\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sys\n",
        "import gymnasium\n",
        "sys.modules[\"gym\"] = gymnasium\n",
        "\n",
        "import stable_baselines3\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.results_plotter import ts2xy, load_results\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from stable_baselines3.common.env_util import make_atari_env\n",
        "\n",
        "import gymnasium as gym\n",
        "from gym import spaces\n",
        "from gym.envs.box2d.lunar_lander import *\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn_layers = [64, 64]  # This is the configuration of your neural network. Currently, we have two layers, each consisting of 64 neurons.\n",
        "                      # If you want three layers with 64 neurons each, set the value to [64,64,64] and so on.\n",
        "\n",
        "learning_rate = 0.001  # This is the step-size with which the gradient descent"
      ],
      "metadata": {
        "id": "ch5wNOJs6LS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_dir = \"/tmp/gym/\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "# Create environment\n",
        "env_name = 'LunarLander-v2'\n",
        "env = gym.make(env_name)\n",
        "# You can also load other environments like cartpole, MountainCar, Acrobot.\n",
        "# Refer to https://gym.openai.com/docs/ for descriptions.\n",
        "\n",
        "# For example, if you would like to load Cartpole,\n",
        "# just replace the above statement with \"env = gym.make('CartPole-v1')\".\n",
        "\n",
        "env = stable_baselines3.common.monitor.Monitor(env, log_dir )\n",
        "\n",
        "callback = EvalCallback(env, log_path=log_dir, deterministic=True)  # For evaluating the performance of the agent periodically and logging the results.\n",
        "policy_kwargs = dict(activation_fn=torch.nn.ReLU,\n",
        "                     net_arch=nn_layers)\n",
        "model = DQN(\"MlpPolicy\", env,policy_kwargs = policy_kwargs,\n",
        "            learning_rate=learning_rate,\n",
        "            batch_size=1,  # for simplicity, we are not doing batch update.\n",
        "            buffer_size=1,  # size of experience of replay buffer. Set to 1 as batch update is not done\n",
        "            learning_starts=1,  # learning starts immediately!\n",
        "            gamma=0.99,  # discount facto. range is between 0 and 1.\n",
        "            tau = 1,  # the soft update coefficient for updating the target network\n",
        "            target_update_interval=1,  # update the target network immediately.\n",
        "            train_freq=(1,\"step\"),  # train the network at every step.\n",
        "            max_grad_norm = 10,  # the maximum value for the gradient clipping\n",
        "            exploration_initial_eps = 1,  # initial value of random action probability\n",
        "            exploration_fraction = 0.5,  # fraction of entire training period over which the exploration rate is reduced\n",
        "            gradient_steps = 1,  # number of gradient steps\n",
        "            seed = 1,  # seed for the pseudo random generators\n",
        "            verbose=0)  # Set verbose to 1 to observe training logs. We encourage you to set the verbose to 1.\n",
        "\n",
        "# You can also experiment with other RL algorithms like A2C, PPO, DDPG etc.\n",
        "# Refer to  https://stable-baselines3.readthedocs.io/en/master/guide/examples.html\n",
        "# for documentation. For example, if you would like to run DDPG, just replace \"DQN\" above with \"DDPG\"."
      ],
      "metadata": {
        "id": "1crEDbib6P20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The input state of the Lunar Lander consists of following components:\n",
        "\n",
        "Horizontal Position\n",
        "Vertical Position\n",
        "Horizontal Velocity\n",
        "Vertical Velocity\n",
        "Angle\n",
        "Angular Velocity\n",
        "Left Leg Contact\n",
        "Right Leg Contact\n",
        "The actions of the agents are:\n",
        "\n",
        "Do Nothing\n",
        "Fire Main Engine\n",
        "Fire Left Engine\n",
        "Fire Right Engine"
      ],
      "metadata": {
        "id": "dUh1wkaH6ciI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = 'LunarLander-v2'\n",
        "env = gym.make(env_name)\n",
        "print('State shape: ', env.observation_space.shape)\n",
        "print('Number of actions: ', env.action_space.n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJtLlizYjtlh",
        "outputId": "4a804551-a42a-4605-d4cc-fc8396f0fcfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State shape:  (8,)\n",
            "Number of actions:  4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QX5m8MJbjt8J"
      },
      "outputs": [],
      "source": [
        "env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "vid = VideoRecorder(env, path=f\"video/{env_name}_pretraining.mp4\")\n",
        "\n",
        "observation = env.reset()[0]\n",
        "total_reward = 0\n",
        "done = False\n",
        "while not done:\n",
        "  frame = env.render()\n",
        "  vid.capture_frame()\n",
        "  action, states = model.predict(observation, deterministic=True)\n",
        "  observation, reward, done, info, _ = env.step(action)\n",
        "  total_reward += reward\n",
        "vid.close()\n",
        "env.close()\n",
        "print(f\"\\nTotal reward: {total_reward}\")\n",
        "\n",
        "# show video\n",
        "html = render_mp4(f\"video/{env_name}_pretraining.mp4\")\n",
        "HTML(html)"
      ]
    }
  ]
}